{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:37:17.451057Z",
     "start_time": "2025-04-09T14:36:32.155020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!pip install google-play-scraper\n",
    "!pip install sastrawi\n",
    "!pip install emoji\n",
    "!pip install imbalanced-learn\n",
    "!pip install nltk"
   ],
   "id": "538488d813002da2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-play-scraper in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (1.2.7)\n",
      "Requirement already satisfied: sastrawi in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (1.0.1)\n",
      "Requirement already satisfied: emoji in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (2.14.1)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (from imbalanced-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (from imbalanced-learn) (1.13.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (from imbalanced-learn) (3.6.0)\n",
      "Requirement already satisfied: nltk in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rucirasatti n\\dataspellprojects\\latpengml\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:38:27.998613Z",
     "start_time": "2025-04-09T14:37:17.501490Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import nltk\n",
    "import emoji\n",
    "from functools import lru_cache\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Download resource NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ],
   "id": "55ef2bc51722dd07",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Rucirasatti\n",
      "[nltk_data]     N\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Rucirasatti\n",
      "[nltk_data]     N\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pra-pemrosesan Data\n",
    "Lakukan pra-pemrosesan data untuk membersihkan teks, menghapus duplikat, dan menerapkan langkah-langkah seperti cleaning, casefolding, slangword correction, tokenizing, stopword removal, stemming, dll.\n"
   ],
   "id": "f51e3ec4163d0cf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:38:30.153546Z",
     "start_time": "2025-04-09T14:38:29.540565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Langkah 2: Pra-pemrosesan Data\n",
    "print(\"\\nLangkah 2: Pra-pemrosesan Data\")\n",
    "df = pd.read_csv('ulasan_whatsapp.csv')\n",
    "clean_df = df.dropna().drop_duplicates()\n",
    "\n",
    "# Menampilkan informasi tentang DataFrame\n",
    "print(\"Informasi DataFrame setelah pembersihan awal:\")\n",
    "clean_df.info()\n"
   ],
   "id": "7fee34a34e5683f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Langkah 2: Pra-pemrosesan Data\n",
      "Informasi DataFrame setelah pembersihan awal:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 62895 entries, 0 to 62999\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   content  62895 non-null  object\n",
      "dtypes: object(1)\n",
      "memory usage: 982.7+ KB\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Definisikan Fungsi Preprocessing\n",
    "Definisikan fungsi untuk cleaning, casefolding, tokenizing, stopword removal, stemming, dan penanganan slangword.\n"
   ],
   "id": "be72bf19a85acca4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:38:30.236798Z",
     "start_time": "2025-04-09T14:38:30.169631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Langkah 3: Definisikan fungsi preprocessing\n",
    "def convert_emojis(text):\n",
    "    return emoji.demojize(text, language='en')\n",
    "\n",
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # menghapus mention\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text) # menghapus hashtag\n",
    "    text = re.sub(r'RT[\\s]', '', text) # menghapus RT\n",
    "    text = re.sub(r\"http\\S+\", '', text) # menghapus link\n",
    "    text = re.sub(r'[0-9]+', '', text) # menghapus angka\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # menghapus karakter selain huruf dan angka\n",
    "    text = text.replace('\\n', ' ') # mengganti baris baru dengan spasi\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # menghapus semua tanda baca\n",
    "    text = text.strip(' ') # menghapus karakter spasi dari kiri dan kanan teks\n",
    "    return text\n",
    "\n",
    "def normalize_repeated_chars(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "def casefoldingText(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def translate_english_words(text):\n",
    "    english_to_indonesian = {\n",
    "        \"history\": \"riwayat\", \"video\": \"video\", \"photo\": \"foto\", \"status\": \"status\",\n",
    "        \"update\": \"pembaruan\", \"bug\": \"kesalahan\", \"feature\": \"fitur\", \"chat\": \"obrolan\",\n",
    "        \"story\": \"cerita\", \"blur\": \"buram\", \"hd\": \"kualitas tinggi\"\n",
    "    }\n",
    "    words = text.split()\n",
    "    translated_words = [english_to_indonesian.get(word.lower(), word) for word in words]\n",
    "    return ' '.join(translated_words)\n",
    "\n",
    "slangwords = {\n",
    "    \"@\": \"di\", \"abis\": \"habis\", \"wtb\": \"beli\", \"masi\": \"masih\", \"wts\": \"jual\", \"wtt\": \"tukar\",\n",
    "    \"bgt\": \"banget\", \"maks\": \"maksimal\", \"pdhl\": \"padahal\", \"agk\": \"agak\", \"sw\": \"status\",\n",
    "    \"burik\": \"buram\", \"ga\": \"tidak\", \"gak\": \"tidak\", \"gk\": \"tidak\", \"nggak\": \"tidak\",\n",
    "    \"tp\": \"tapi\", \"klo\": \"kalau\", \"kl\": \"kalau\", \"udh\": \"sudah\", \"sdh\": \"sudah\",\n",
    "    \"bkin\": \"bikin\", \"bkn\": \"bukan\", \"krn\": \"karena\", \"krna\": \"karena\", \"bs\": \"bisa\",\n",
    "    \"blm\": \"belum\", \"sm\": \"sama\", \"sma\": \"sama\", \"gt\": \"gitu\", \"gtu\": \"gitu\",\n",
    "    \"jgn\": \"jangan\", \"yg\": \"yang\", \"kpn\": \"kapan\", \"knp\": \"kenapa\", \"knpa\": \"kenapa\",\n",
    "    \"org\": \"orang\", \"dr\": \"dari\", \"dri\": \"dari\", \"ttg\": \"tentang\", \"hrs\": \"harus\",\n",
    "    \"lg\": \"lagi\", \"lgi\": \"lagi\", \"cm\": \"cuma\", \"cma\": \"cuma\", \"aj\": \"aja\", \"ajh\": \"aja\",\n",
    "    \"doang\": \"saja\", \"bgs\": \"bagus\", \"bgus\": \"bagus\", \"skrg\": \"sekarang\", \"skg\": \"sekarang\",\n",
    "    \"nnti\": \"nanti\", \"nt\": \"nanti\", \"smpe\": \"sampai\", \"smp\": \"sampai\", \"trh\": \"terus\",\n",
    "    \"trs\": \"terus\", \"tpi\": \"tapi\", \"bnyk\": \"banyak\", \"bnk\": \"banyak\", \"sblm\": \"sebelum\",\n",
    "    \"stlh\": \"setelah\", \"slh\": \"salah\", \"ksh\": \"kasih\", \"thx\": \"terima kasih\", \"makasih\": \"terima kasih\",\n",
    "    \"vc\": \"video call\", \"vn\": \"voice note\", \"hp\": \"handphone\", \"apk\": \"aplikasi\", \"min\": \"admin\"\n",
    "}\n",
    "\n",
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text\n",
    "\n",
    "def tokenizingText(text):\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def filteringText(text):\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords1 = set(stopwords.words('english'))\n",
    "    listStopwords.update(listStopwords1)\n",
    "    listStopwords.update(['iya', 'yaa', 'sih', 'ku', 'gaa', 'loh', 'kah', 'woi', 'woii', 'woy'])\n",
    "    listStopwords.difference_update({'tidak', 'gak', 'nggak', 'bukan'})\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered\n",
    "    return text\n",
    "\n",
    "# Optimasi stemming dengan cache\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def cached_stem(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def stemmingText(text):\n",
    "    words = text.split()\n",
    "    stemmed_words = [cached_stem(word) for word in words]\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "    return stemmed_text\n",
    "\n",
    "def toSentence(list_words):\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence\n",
    "\n",
    "def remove_repeated_words(text):\n",
    "    words = text.split()\n",
    "    result = []\n",
    "    prev_word = None\n",
    "    for word in words:\n",
    "        if word != prev_word:\n",
    "            result.append(word)\n",
    "            prev_word = word\n",
    "    return ' '.join(result)\n"
   ],
   "id": "b3a20391c1649d15",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Terapkan Preprocessing\n",
    "Terapkan langkah-langkah preprocessing pada DataFrame.\n"
   ],
   "id": "f8e6065d9a729121"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:42:20.298125Z",
     "start_time": "2025-04-09T14:38:30.314526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Langkah 4: Terapkan preprocessing\n",
    "# 1. Konversi emoticon ke teks\n",
    "clean_df['text_emoticon'] = clean_df['content'].apply(convert_emojis)\n",
    "\n",
    "# 2. Membersihkan teks\n",
    "clean_df['text_clean'] = clean_df['text_emoticon'].apply(cleaningText)\n",
    "\n",
    "# 3. Normalisasi karakter berulang\n",
    "clean_df['text_normalized'] = clean_df['text_clean'].apply(normalize_repeated_chars)\n",
    "\n",
    "# 4. Mengubah huruf menjadi huruf kecil\n",
    "clean_df['text_casefoldingText'] = clean_df['text_normalized'].apply(casefoldingText)\n",
    "\n",
    "# 5. Menerjemahkan kata-kata Inggris ke Indonesia\n",
    "clean_df['text_translated'] = clean_df['text_casefoldingText'].apply(translate_english_words)\n",
    "\n",
    "# 6. Mengganti kata-kata slang\n",
    "clean_df['text_slangwords'] = clean_df['text_translated'].apply(fix_slangwords)\n",
    "\n",
    "# 7. Memecah teks menjadi token\n",
    "clean_df['text_tokenizingText'] = clean_df['text_slangwords'].apply(tokenizingText)\n",
    "\n",
    "# 8. Menghapus stopwords\n",
    "clean_df['text_stopword'] = clean_df['text_tokenizingText'].apply(filteringText)\n",
    "\n",
    "# 9. Menggabungkan token menjadi kalimat\n",
    "clean_df['text_akhir'] = clean_df['text_stopword'].apply(toSentence)\n",
    "\n",
    "# 10. Stemming\n",
    "# clean_df['text_stemmed'] = clean_df['text_akhir'].apply(stemmingText)\n",
    "\n",
    "# 11. Menghapus kata berulang berurutan\n",
    "clean_df['text_final'] = clean_df['text_akhir'].apply(remove_repeated_words)\n"
   ],
   "id": "340556b3acb0bac0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Pelabelan Sentimen Menggunakan Lexicon\n",
    "Gunakan kamus lexicon positif dan negatif untuk menentukan polaritas sentimen (positif, netral, negatif).\n"
   ],
   "id": "6e530e3f9abffca1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:42:30.074954Z",
     "start_time": "2025-04-09T14:42:20.380449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Langkah 5: Pelabelan Sentimen Menggunakan Lexicon\n",
    "print(\"\\nLangkah 5: Pelabelan Sentimen Menggunakan Lexicon\")\n",
    "\n",
    "# Membaca data kamus kata-kata positif dari GitHub\n",
    "lexicon_positive = dict()\n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
    "if response.status_code == 200:\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    for row in reader:\n",
    "        lexicon_positive[row[0]] = int(row[1])\n",
    "else:\n",
    "    print(\"Failed to fetch positive lexicon data\")\n",
    "\n",
    "# Membaca data kamus kata-kata negatif dari GitHub\n",
    "lexicon_negative = dict()\n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')\n",
    "if response.status_code == 200:\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    for row in reader:\n",
    "        lexicon_negative[row[0]] = int(row[1])\n",
    "else:\n",
    "    print(\"Failed to fetch negative lexicon data\")\n",
    "\n",
    "# Fungsi untuk menentukan polaritas sentimen\n",
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    score = 0\n",
    "    for word in text:\n",
    "        if word in lexicon_positive:\n",
    "            score += lexicon_positive[word]\n",
    "        if word in lexicon_negative:\n",
    "            score += lexicon_negative[word]\n",
    "\n",
    "    polarity = ''\n",
    "    if score >= 7:\n",
    "        polarity = 'positive'\n",
    "    elif score == 3:\n",
    "        polarity = 'neutral'\n",
    "    else:\n",
    "        polarity = 'negative'\n",
    "\n",
    "    return score, polarity\n",
    "\n",
    "# Terapkan analisis sentimen\n",
    "results = clean_df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "results = list(zip(*results))\n",
    "clean_df['polarity_score'] = results[0]\n",
    "clean_df['polarity'] = results[1]\n",
    "\n",
    "print(\"Distribusi polaritas:\")\n",
    "print(clean_df['polarity'].value_counts())\n",
    "clean_df.to_csv('processed_ulasan_whatsapp.csv', index=False)\n"
   ],
   "id": "d3cb9cb6a4fd71a7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Langkah 5: Pelabelan Sentimen Menggunakan Lexicon\n",
      "Distribusi polaritas:\n",
      "polarity\n",
      "negative    54852\n",
      "positive     5814\n",
      "neutral      2229\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Skema Pelatihan\n",
    "Lakukan tiga skema pelatihan:\n",
    "- Skema 1: Decision Tree + TF-IDF\n",
    "- Skema 2: Random Forest + TF-IDF\n",
    "- Skema 3: Deep Learning (CNN)\n"
   ],
   "id": "aff54b7e3ace702e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:44:49.436364Z",
     "start_time": "2025-04-09T14:42:30.097710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nSkema 1: Decision Tree + TF-IDF, Pembagian Data 80/20\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_df['text_final'], clean_df['polarity'], test_size=0.20, random_state=42)\n",
    "\n",
    "vectorizer_tfidf = TfidfVectorizer(\n",
    "    max_features=30000,\n",
    "    ngram_range=(1,2),\n",
    "    sublinear_tf=True,\n",
    "    use_idf=True,\n",
    ")\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train.fillna(\"\"))\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test.fillna(\"\"))\n",
    "\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_tfidf, y_train = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=60, min_samples_leaf=2, random_state=42)\n",
    "dt.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_dt_train = dt.predict(X_train_tfidf)\n",
    "y_pred_dt_test = dt.predict(X_test_tfidf)\n",
    "accuracy_dt_train = round(accuracy_score(y_train, y_pred_dt_train) * 100)\n",
    "accuracy_dt_test = round(accuracy_score(y_test, y_pred_dt_test) * 100)\n",
    "print(f\"Akurasi Skema 1 (Decision Tree + TF-IDF) - Training: {accuracy_dt_train}%\")\n",
    "print(f\"Akurasi Skema 1 (Decision Tree + TF-IDF) - Testing: {accuracy_dt_test}%\")"
   ],
   "id": "12bf64983007530f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skema 1: Decision Tree + TF-IDF, Pembagian Data 80/20\n",
      "Akurasi Skema 1 (Decision Tree + TF-IDF) - Training: 98%\n",
      "Akurasi Skema 1 (Decision Tree + TF-IDF) - Testing: 85%\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:54:25.050563Z",
     "start_time": "2025-04-09T14:44:49.566439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nSkema 2: Random Forest + TF-IDF, Pembagian Data 75/25\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(clean_df['text_final'], clean_df['polarity'], test_size=0.25, random_state=42)\n",
    "\n",
    "X_train_tfidf = vectorizer_tfidf.fit_transform(X_train.fillna(\"\"))\n",
    "X_test_tfidf = vectorizer_tfidf.transform(X_test.fillna(\"\"))\n",
    "\n",
    "X_train_tfidf, y_train = smote.fit_resample(X_train_tfidf, y_train)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400, max_depth=60, min_samples_leaf=2, random_state=42)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "y_pred_rf_train = rf.predict(X_train_tfidf)\n",
    "y_pred_rf_test = rf.predict(X_test_tfidf)\n",
    "accuracy_rf_train = round(accuracy_score(y_train, y_pred_rf_train) * 100)\n",
    "accuracy_rf_test = round(accuracy_score(y_test, y_pred_rf_test) * 100)\n",
    "print(f\"Akurasi Skema 2 (Random Forest + TF-IDF) - Training: {accuracy_rf_train}%\")\n",
    "print(f\"Akurasi Skema 2 (Random Forest + TF-IDF) - Testing: {accuracy_rf_test}%\")"
   ],
   "id": "5cb690a9dd32ac78",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skema 2: Random Forest + TF-IDF, Pembagian Data 75/25\n",
      "Akurasi Skema 2 (Random Forest + TF-IDF) - Training: 97%\n",
      "Akurasi Skema 2 (Random Forest + TF-IDF) - Testing: 89%\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:54:29.718233Z",
     "start_time": "2025-04-09T14:54:25.152553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nSkema 3: Deep Learning (CNN), Pembagian Data 80/20\")\n",
    "\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(clean_df['text_final'])\n",
    "sequences = tokenizer.texts_to_sequences(clean_df['text_final'])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "label_mapping = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "clean_df['label'] = clean_df['polarity'].map(label_mapping)\n",
    "\n",
    "X = padded_sequences\n",
    "y = to_categorical(clean_df['label'], num_classes=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Jumlah data training: {len(X_train)}\")\n",
    "print(f\"Jumlah data testing: {len(X_test)}\")"
   ],
   "id": "840f20ffe02f1d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Skema 3: Deep Learning (CNN), Pembagian Data 80/20\n",
      "Jumlah data training: 50316\n",
      "Jumlah data testing: 12579\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:56:39.712642Z",
     "start_time": "2025-04-09T14:54:29.758847Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "\n",
    "embedding_dim = 100\n",
    "model_cnn = Sequential([\n",
    "    Embedding(max_words, embedding_dim, input_length=max_len),\n",
    "    Conv1D(256, 3, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_cnn.summary()\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "history_cnn = model_cnn.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")"
   ],
   "id": "8d02e4d9f6070305",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rucirasatti N\\DataspellProjects\\latpengml\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"sequential\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001B[38;5;33mEmbedding\u001B[0m)           │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (\u001B[38;5;33mConv1D\u001B[0m)                 │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalMaxPooling1D\u001B[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001B[38;5;33mDense\u001B[0m)                   │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001B[38;5;33mDropout\u001B[0m)               │ ?                      │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ ?                      │   \u001B[38;5;34m0\u001B[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_max_pooling1d            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1D</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m0\u001B[0m (0.00 B)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001B[1m787/787\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m30s\u001B[0m 34ms/step - accuracy: 0.8719 - loss: 0.4119 - val_accuracy: 0.9202 - val_loss: 0.2379\n",
      "Epoch 2/20\n",
      "\u001B[1m787/787\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m26s\u001B[0m 32ms/step - accuracy: 0.9245 - loss: 0.2192 - val_accuracy: 0.9238 - val_loss: 0.2248\n",
      "Epoch 3/20\n",
      "\u001B[1m787/787\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m25s\u001B[0m 32ms/step - accuracy: 0.9421 - loss: 0.1628 - val_accuracy: 0.9235 - val_loss: 0.2540\n",
      "Epoch 4/20\n",
      "\u001B[1m787/787\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 31ms/step - accuracy: 0.9525 - loss: 0.1244 - val_accuracy: 0.9216 - val_loss: 0.2955\n",
      "Epoch 5/20\n",
      "\u001B[1m787/787\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m24s\u001B[0m 30ms/step - accuracy: 0.9567 - loss: 0.0999 - val_accuracy: 0.9222 - val_loss: 0.3049\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:57:38.435313Z",
     "start_time": "2025-04-09T14:57:26.349049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y_pred_train = model_cnn.predict(X_train)\n",
    "y_pred_test = model_cnn.predict(X_test)\n",
    "y_pred_train_labels = np.argmax(y_pred_train, axis=1)\n",
    "y_pred_test_labels = np.argmax(y_pred_test, axis=1)\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "accuracy_cnn_train = round(accuracy_score(y_train_labels, y_pred_train_labels) * 100)\n",
    "accuracy_cnn_test = round(accuracy_score(y_test_labels, y_pred_test_labels) * 100)\n",
    "print(f\"Akurasi Skema 3 (CNN) - Training: {accuracy_cnn_train}%\")\n",
    "print(f\"Akurasi Skema 3 (CNN) - Testing: {accuracy_cnn_test}%\")"
   ],
   "id": "b024097563007dfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1573/1573\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 5ms/step\n",
      "\u001B[1m394/394\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m2s\u001B[0m 5ms/step\n",
      "Akurasi Skema 3 (CNN) - Training: 95%\n",
      "Akurasi Skema 3 (CNN) - Testing: 92%\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hasil Analisis Sentimen",
   "id": "6272679104e8a386"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:02:30.075564Z",
     "start_time": "2025-04-09T15:02:30.059090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dictionary untuk menyimpan hasil akurasi\n",
    "accuracy_results = {\n",
    "    'Skema': [\n",
    "        'Decision Tree + TF-IDF',\n",
    "        'Random Forest + TF-IDF',\n",
    "        'CNN'\n",
    "    ],\n",
    "    'Akurasi Training (%)': [\n",
    "        f\"{int(accuracy_dt_train)}%\",\n",
    "        f\"{int(accuracy_rf_train)}%\",\n",
    "        f\"{int(accuracy_cnn_train)}%\"\n",
    "    ],\n",
    "    'Akurasi Testing (%)': [\n",
    "        f\"{int(accuracy_dt_test)}%\",\n",
    "        f\"{int(accuracy_rf_test)}%\",\n",
    "        f\"{int(accuracy_cnn_test)}%\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Buat DataFrame dari dictionary\n",
    "accuracy_df = pd.DataFrame(accuracy_results)\n",
    "\n",
    "# Tampilkan tabel\n",
    "print(\"\\nPerbandingan Akurasi dari Ketiga Skema:\")\n",
    "print(accuracy_df.to_string(index=False))"
   ],
   "id": "20afea93db59aa91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perbandingan Akurasi dari Ketiga Skema:\n",
      "                 Skema Akurasi Training (%) Akurasi Testing (%)\n",
      "Decision Tree + TF-IDF                  98%                 85%\n",
      "Random Forest + TF-IDF                  97%                 89%\n",
      "                   CNN                  95%                 92%\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Inference\n",
    "Lakukan inference pada beberapa sampel data untuk melihat hasil prediksi dari model."
   ],
   "id": "14355e990bc69b45"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:03:16.450917Z",
     "start_time": "2025-04-09T15:03:16.358792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Langkah 7: Inference untuk Testing\n",
    "print(\"\\nLangkah 7: Inference untuk Testing (Menggunakan Model Skema 2 - Random Forest)\")\n",
    "# Ambil 5 sampel acak dari data testing untuk inference\n",
    "sample_indices = np.random.choice(X_test.shape[0], 5, replace=False)\n",
    "sample_texts = X_test[sample_indices]\n",
    "sample_labels = y_test[sample_indices]\n",
    "\n",
    "# Untuk Random Forest, kita perlu mengubah kembali teks ke format TF-IDF\n",
    "sample_texts_raw = clean_df['text_final'].iloc[sample_indices]\n",
    "sample_texts_tfidf = vectorizer_tfidf.transform(sample_texts_raw.fillna(\"\"))\n",
    "\n",
    "# Prediksi menggunakan model Random Forest (Skema 2)\n",
    "predictions_rf = rf.predict(sample_texts_tfidf)\n",
    "true_labels = np.argmax(sample_labels, axis=1)\n",
    "\n",
    "# Mapping kembali ke label kategorikal\n",
    "reverse_label_mapping = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
    "predicted_labels_text = [predictions_rf[i] for i in range(len(predictions_rf))]\n",
    "true_labels_text = [reverse_label_mapping[label] for label in true_labels]"
   ],
   "id": "2c0b3bdc65d5c416",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Langkah 7: Inference untuk Testing (Menggunakan Model Skema 2 - Random Forest)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:03:18.975534Z",
     "start_time": "2025-04-09T15:03:18.962670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Tampilkan hasil inference\n",
    "print(\"\\nHasil Inference (Random Forest):\")\n",
    "for i in range(len(sample_texts_raw)):\n",
    "    print(f\"Sampel {i+1}:\")\n",
    "    print(f\"Teks: {sample_texts_raw.iloc[i]}\")\n",
    "    print(f\"Prediksi: {predicted_labels_text[i]}\")\n",
    "    print(f\"Label Sebenarnya: {true_labels_text[i]}\\n\")"
   ],
   "id": "c2f7d7472ecd4976",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hasil Inference (Random Forest):\n",
      "Sampel 1:\n",
      "Teks: senang mengunakan whatsap kena spam nomer masukan salah tidak mengunakan whatsap tergantung november februari\n",
      "Prediksi: negative\n",
      "Label Sebenarnya: positive\n",
      "\n",
      "Sampel 2:\n",
      "Teks: aplikasi nya dijadikan perangkat pendamping eror tidak login cek koneksi internet normal sped mbs\n",
      "Prediksi: negative\n",
      "Label Sebenarnya: negative\n",
      "\n",
      "Sampel 3:\n",
      "Teks: tolong pengiriman video status sesuai kualitas kamera bikin cerita wa gambar video pecah tolong perbaiki\n",
      "Prediksi: negative\n",
      "Label Sebenarnya: negative\n",
      "\n",
      "Sampel 4:\n",
      "Teks: bagus temen tidak nge priv p nya tidak p kalo status burem tidak jelasterus permasalahan voice note teriak ngomong deketin kekipas tetep aja tidak suara jg temen kadang gitu tolong diperbaiki cape whatsap fitur ngebug\n",
      "Prediksi: negative\n",
      "Label Sebenarnya: negative\n",
      "\n",
      "Sampel 5:\n",
      "Teks: perbaharuan handphone skli pdhal otomatis lerbaharui aja slu\n",
      "Prediksi: negative\n",
      "Label Sebenarnya: negative\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T15:03:23.443491Z",
     "start_time": "2025-04-09T15:03:23.195553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inference menggunakan model CNN (Skema 3)\n",
    "print(\"\\nLangkah 7: Inference untuk Testing (Menggunakan Model Skema 3 - CNN)\")\n",
    "predictions_cnn = model_cnn.predict(sample_texts)\n",
    "predicted_labels_cnn = np.argmax(predictions_cnn, axis=1)\n",
    "predicted_labels_cnn_text = [reverse_label_mapping[label] for label in predicted_labels_cnn]\n",
    "\n",
    "# Tampilkan hasil inference\n",
    "print(\"\\nHasil Inference (CNN):\")\n",
    "for i in range(len(sample_texts_raw)):\n",
    "    print(f\"Sampel {i+1}:\")\n",
    "    print(f\"Teks: {sample_texts_raw.iloc[i]}\")\n",
    "    print(f\"Prediksi: {predicted_labels_cnn_text[i]}\")\n",
    "    print(f\"Label Sebenarnya: {true_labels_text[i]}\\n\")\n",
    "\n",
    "print(\"\\nProses selesai!\")"
   ],
   "id": "1cb5401fc2ef0dc8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Langkah 7: Inference untuk Testing (Menggunakan Model Skema 3 - CNN)\n",
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 134ms/step\n",
      "\n",
      "Hasil Inference (CNN):\n",
      "Sampel 1:\n",
      "Teks: senang mengunakan whatsap kena spam nomer masukan salah tidak mengunakan whatsap tergantung november februari\n",
      "Prediksi: positive\n",
      "Label Sebenarnya: positive\n",
      "\n",
      "Sampel 2:\n",
      "Teks: aplikasi nya dijadikan perangkat pendamping eror tidak login cek koneksi internet normal sped mbs\n",
      "Prediksi: negative\n",
      "Label Sebenarnya: negative\n",
      "\n",
      "Sampel 3:\n",
      "Teks: tolong pengiriman video status sesuai kualitas kamera bikin cerita wa gambar video pecah tolong perbaiki\n",
      "Prediksi: negative\n",
      "Label Sebenarnya: negative\n",
      "\n",
      "Sampel 4:\n",
      "Teks: bagus temen tidak nge priv p nya tidak p kalo status burem tidak jelasterus permasalahan voice note teriak ngomong deketin kekipas tetep aja tidak suara jg temen kadang gitu tolong diperbaiki cape whatsap fitur ngebug\n",
      "Prediksi: negative\n",
      "Label Sebenarnya: negative\n",
      "\n",
      "Sampel 5:\n",
      "Teks: perbaharuan handphone skli pdhal otomatis lerbaharui aja slu\n",
      "Prediksi: negative\n",
      "Label Sebenarnya: negative\n",
      "\n",
      "\n",
      "Proses selesai!\n"
     ]
    }
   ],
   "execution_count": 23
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
